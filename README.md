Source code for the paper:

Finetuned Language Models are Zero-Shot Learners, Wei et. al., 2021.
